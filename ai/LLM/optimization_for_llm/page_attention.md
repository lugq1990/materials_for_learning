## pageattention


### [what is the core of pageattetion based on vLLM docs](https://blog.vllm.ai/2023/06/20/vllm.html)

PagedAttention 是 vLLM 库中的一个关键技术，它是一种新的注意力算法，用于提高大型语言模型（LLM）推理和服务的效率。以下是对 PagedAttention 核心特性的总结：

1. **内存管理**：
   - PagedAttention 通过有效管理注意力机制中的键（key）和值（value）的内存使用，解决了性能瓶颈问题。

2. **内存效率**：
   - 在自回归解码过程中，所有输入令牌都会产生它们的注意力键和值张量，这些张量通常存储在 GPU 内存中以生成下一个令牌。PagedAttention 通过将键值缓存（KV cache）分割成多个块来提高内存效率。

3. **非连续存储**：
   - 与传统注意力算法不同，PagedAttention 允许在非连续的内存空间中存储连续的键和值。

4. **块状管理**：
   - PagedAttention 将每个序列的 KV 缓存分割成块，每个块包含固定数量的令牌的键和值。在注意力计算期间，PagedAttention 内核能够高效地识别和获取这些块。

5. **虚拟内存和分页**：
   - PagedAttention 的设计灵感来自操作系统中的虚拟内存和分页概念。它通过块表将序列的连续逻辑块映射到非连续的物理块。

6. **内存浪费最小化**：
   - 在 PagedAttention 中，内存浪费仅发生在序列的最后一个块中。在实践中，这导致接近最优的内存使用，浪费不到 4%。

7. **内存共享**：
   - PagedAttention 通过其块表自然地支持内存共享，类似于操作系统中进程共享物理页面的方式。

8. **并行采样**：
   - 在并行采样中，可以基于同一提示生成多个输出序列。PagedAttention 可以共享提示的计算和内存，从而减少复杂采样算法的内存开销。

9. **性能提升**：
   - 通过提高内存效率和支持内存共享，PagedAttention 大大提高了 LLM 服务的吞吐量，例如，它可以将并行采样和束搜索的内存使用减少高达 55%，从而将吞吐量提高多达 2.2 倍。

10. **核心技术**：
    - PagedAttention 是 vLLM 的核心技术，它是一个支持多种模型的 LLM 推理和服务引擎，具有高性能和易于使用的接口。

PagedAttention 的创新之处在于它解决了大型语言模型在服务时面临的内存管理挑战，通过优化内存使用和支持高效的内存共享，显著提高了模型服务的吞吐量和效率。